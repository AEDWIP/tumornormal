{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "by Andrew E. Davidson\n",
    "\n",
    "This notebook is an introduction to TensorFlow. To illustrate how basic TensorFlow works we are going to implement binary classifier using Logistic Regression. Logistic Regression can be thought of as neural network with a single neuron. Its simple enough that we can implement the algorithm manually. With the interest of learning how tensor flow works we are not going to take advantage of automatic differentiation or tensoflow optimizers. Using auto-diff and optimizers would dramatically reduce the amount of code required.\n",
    "\n",
    "Note: [train.ipynb](train.ipynb) is a version of logistic regression using Keras. Its a fraction of the code a TensorFlow only solution requires.\n",
    "\n",
    "<span style=\"color:red\">There are couple of place where I used numpy reshape or to scale the feature matrix. This should be changed to use TensorFlow. Using numpy will work if you are using a single machine. Tensorflow will scale across a cluster.</span>\n",
    "    \n",
    "Supervised learning applications implemented with TensorFlow typically have the following structure\n",
    "1. load some data\n",
    "\n",
    "2. construct our 'computation graph'\n",
    "    1. Initialize our input variables, model parameters, and place holders\n",
    "    1. Forward Propagation\n",
    "        * create graph that makes predictions give our features and model\n",
    "    2. Calculate Cost\n",
    "        * create graph to measure how well our preidiction match our lables\n",
    "    3. Back Propagation\n",
    "        * calculate the graidents. (i.e. the partical derivatites of our cost function with respect to the partameters our model learns\n",
    "    4. use the gradients to update the learned parameters of our model\n",
    "3. Train our model\n",
    "    1. Use an optimizer or gradient decent with mini-batches\n",
    "    2. save check points and final model\n",
    "    3. save performance statistics. \n",
    "4. evaluate our model's performance on the training data set\n",
    "5. evaluate our model's performance on the test data set\n",
    "6. tune hyper parmeters \n",
    "7. goto to 1.\n",
    "\n",
    "### TODO\n",
    "- save check point and model\n",
    "- run on large data set\n",
    "- does model generalize?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import logRegTestFunc as lrTest\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from scipy import stats\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Load RNA Seq data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.io_utils import HDF5Matrix\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_data_set(input_file):\n",
    "    print(\"Training on full dataset\")\n",
    "    X_train = HDF5Matrix(input_file, \"X_train\")\n",
    "    X_test = HDF5Matrix(input_file, \"X_test\")\n",
    "    y_train = HDF5Matrix(input_file, \"y_train\")\n",
    "    y_test = HDF5Matrix(input_file, \"y_test\")\n",
    "    \n",
    "    return [X_train, X_test, y_train, y_test]\n",
    "\n",
    "def small_data_set(input_file):\n",
    "    print(\"Training on partial dataset\")\n",
    "    X_train = HDF5Matrix(input_file, \"X_train\", start=0, end=1000)\n",
    "    X_test = HDF5Matrix(input_file, \"X_test\", start=0, end=200)\n",
    "    y_train = HDF5Matrix(input_file, \"y_train\", start=0, end=1000)\n",
    "    y_test = HDF5Matrix(input_file, \"y_test\", start=0, end=200)\n",
    "    \n",
    "    return [X_train, X_test, y_train, y_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets: ['X_test', 'X_train', 'class_labels', 'classes_test', 'classes_train', 'features', 'genes', 'labels', 'y_test', 'y_train']\n",
      "Training on partial dataset\n",
      "X_train.shape:  (60498, 1000)\n",
      "X_test.shape:  (60498, 200)\n",
      "y_train.shape: (1, 1000)\n",
      "y_test.shape: (1, 1000)\n",
      "epochs:  100\n",
      "batch_size:  128\n",
      "n: num features:  60498\n",
      "m: num training samples:  1000\n",
      "CPU times: user 1.28 s, sys: 1.1 s, total: 2.38 s\n",
      "Wall time: 2.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "input_file = \"data/tumor_normal.h5\"\n",
    "\n",
    "with h5py.File(input_file, \"r\") as f:\n",
    "    print(\"Datasets:\", list(f.keys()))\n",
    "    \n",
    "DEBUG = False \n",
    "if DEBUG:\n",
    "    # this data set is simple enough we can do the math on paper\n",
    "    print(\"Training on trival (debug) dataset\")\n",
    "    X_train = np.array([\n",
    "                        [ 1.,  2.],\n",
    "                        [ 3.,  4.],\n",
    "                        [ 5.,  6.],\n",
    "                        [ 7.,  8.],\n",
    "                        [ 9., 10.],\n",
    "                        [11., 12.],\n",
    "                        [13., 14]\n",
    "                            ])\n",
    "    \n",
    "    # y_train must be a rank 2 array\n",
    "    # i.e. we want transpose of y_train to be a column vector not an array\n",
    "    y_train = np.array( [[0., 0., 0., 1., 1., 1., 1.]] )\n",
    "    X_test = X_train * 10\n",
    "    y_test = y_train\n",
    "    \n",
    "    epochs = 5000\n",
    "    batch_size = 3\n",
    "    log_frequence = 100\n",
    "    \n",
    "else:\n",
    "    #X_train, X_test, y_train, y_test = full_data_set(input_file)\n",
    "    X_train, X_test, y_train, y_test = small_data_set(input_file)\n",
    "    \n",
    "    n,m = np.transpose(X_train).shape\n",
    "    m_test = y_test.shape[0]\n",
    "    \n",
    "    # reshape array into a column vector\n",
    "    y_train = np.reshape(y_train, (1,m))\n",
    "    y_test = np.reshape(y_test, (1,m_test))\n",
    "\n",
    "    epochs=100\n",
    "    batch_size=128\n",
    "    log_frequence = epochs / 5 #1 if DEBUG else 20 \n",
    "\n",
    "    \n",
    "# transpose, each column should be a separate sample\n",
    "X_train = np.transpose( X_train )\n",
    "n,m = X_train.shape\n",
    "\n",
    "X_test = np.transpose( X_test )\n",
    "assert X_test.shape == (n, X_test.shape[1]), \"X_test shape failed\"\n",
    "\n",
    "print( \"X_train.shape: \", X_train.shape)\n",
    "print( \"X_test.shape: \", X_test.shape)\n",
    "\n",
    "print(\"y_train.shape:\", y_train.shape)\n",
    "assert y_train.shape == (1,m), \"y_train shape failed\"\n",
    "\n",
    "print(\"y_test.shape:\", y_train.shape)\n",
    "assert y_test.shape == (1, X_test.shape[1])\n",
    "\n",
    "print( \"epochs: \", epochs )\n",
    "print( \"batch_size: \", batch_size )\n",
    "\n",
    "print( \"n: num features: \", n)\n",
    "expectedNumFeatures = 2 if DEBUG else 60498\n",
    "\n",
    "msg = \"expected:\" + str(expectedNumFeatures) + \" actual:\" + str(n)\n",
    "assert (n == expectedNumFeatures), msg\n",
    "\n",
    "print( \"m: num training samples: \", m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore RNA Seq data\n",
    "Given the way the sigmoid function works, the raw data will spend a lot of time learning using gradients near zero. We can speed up learning by normalizing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******** Explore RNA Seq data *******\n",
      "Raw Data, rows are features\n",
      "\n",
      "X_train[0:2,0:8]\n",
      " [[-4.035  -9.9658 -9.9658 -9.9658 -9.9658 -5.0116 -9.9658 -2.4659]\n",
      " [-9.9658 -9.9658 -9.9658 -9.9658 -9.9658 -9.9658 -9.9658 -9.9658]]\n",
      "\n",
      "X_train[100:102,0:8]\n",
      " [[-9.9658 -9.9658 -9.9658 -9.9658 -9.9658 -9.9658 -9.9658 -9.9658]\n",
      " [-0.3201 -3.3076  1.6875  2.6579 -2.6349  2.5338  0.03    6.6964]]\n",
      "\n",
      "y_train[0, 0:8]\n",
      " [0 0 1 1 0 1 1 0]\n",
      "\n",
      "Summary stats for two genes\n",
      "\n",
      "X_train[0,:]\n",
      " minmax:(-9.9658, 2.1894) mean:-6.864265441894531 variance:14.995550155639648\n",
      "\n",
      "X_train[99,:]\n",
      " minmax:(-9.9658, 1.2271) mean:-5.610025405883789 variance:16.54981803894043\n",
      "\n",
      "********** normalizing data ********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:11: RuntimeWarning: invalid value encountered in true_divide\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "def normalizeData(X):\n",
    "    \"\"\"scales data such that mean=0 and standard deviation=1\"\"\"\n",
    "    n,m = X.shape\n",
    "    \n",
    "    # axis=1 cause calculation to run across rows\n",
    "    means = np.mean(X, axis=1).reshape(n, 1)\n",
    "    std = np.std(X, axis=1).reshape(n, 1)\n",
    "    #mean, std = tf.nn.moments(X, axes=[1])\n",
    "    \n",
    "    centered = X - means\n",
    "    return (centered / std)\n",
    "\n",
    "if DEBUG:\n",
    "    testND = np.arange(start=1., stop=13.).reshape(4,3)\n",
    "    result = normalizeData(testND)\n",
    "\n",
    "    assert np.array_equal( np.mean(result, axis=1), \n",
    "                          np.array([0.,0.,0.,0.]) )\n",
    "    assert sum(np.isclose( np.var(result, axis=1), \n",
    "                          np.array([1.,1.,1.,1.]) ), 4)\n",
    "\n",
    "if not DEBUG:\n",
    "    # explore some of the X_train data\n",
    "    # do we need to normalize?\n",
    "    print(\"******** Explore RNA Seq data *******\")\n",
    "    print(\"Raw Data, rows are features\")\n",
    "    print(\"\\nX_train[0:2,0:8]\\n\", X_train[0:2,0:8])\n",
    "    print(\"\\nX_train[100:102,0:8]\\n\", X_train[100:102,0:8])\n",
    "    \n",
    "    print(\"\\ny_train[0, 0:8]\\n\", y_train[0, 0:8])\n",
    "\n",
    "    print(\"\\nSummary stats for two genes\")\n",
    "    s = stats.describe(X_train[0,:])\n",
    "    print(\"\\nX_train[0,:]\\n minmax:{} mean:{} variance:{}\"\n",
    "              .format(s.minmax, s.mean, s.variance) )\n",
    "    \n",
    "    s = stats.describe(X_train[99,:])\n",
    "    print(\"\\nX_train[99,:]\\n minmax:{} mean:{} variance:{}\"\n",
    "              .format(s.minmax, s.mean, s.variance) )  \n",
    "    \n",
    "    print(\"\\n********** normalizing data ********\")\n",
    "    X_train = normalizeData( X_train )\n",
    "    X_test = normalizeData( X_train )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Construction Computation Graph \n",
    "If you want to use TensorBoard to visualize your graph make sure you give all ops a name, and group releated ops using \"name scopes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear out any nodes from previous runs\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "Create a graph that makes predictions given our features and model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init place holders and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape:  (60498, ?)\n",
      "y.shape: (1, ?)\n",
      "w.shape:  (60498, 1)\n",
      "tf.transpose(w).shape:  (1, 60498)\n",
      "b.dtype:  <dtype: 'float64_ref'>\n"
     ]
    }
   ],
   "source": [
    "# We use tf.name_scope() to group tensor operations in the graph image \n",
    "# created by TensorBoard. Use either the \"name\" argument or tf.identity to \n",
    "# lable nodes in the in the TensorBoard Image\n",
    "\n",
    "with tf.name_scope(\"input_data\"):\n",
    "    # shape( None ) means the deminsion is not know. This is because the \n",
    "    # last min batch may have fewer samples. i.e. m / batch_size  may have \n",
    "    # a remainder\n",
    "\n",
    "    # X is our feature matrix\n",
    "    X = tf.placeholder( tf.float64, shape=(n, None), name=\"X\")\n",
    "    print(\"X.shape: \", X.shape)\n",
    "\n",
    "    # y is our lable vector\n",
    "    y = tf.placeholder( tf.float64, shape=(1, None), name=\"y\")\n",
    "    print(\"y.shape:\", y.shape)\n",
    "\n",
    "# initialize our model parameters.\n",
    "# w is our models weight vector\n",
    "# b is the y intercept for a linear model\n",
    "with tf.name_scope(\"model_parameters\"):\n",
    "    if DEBUG:\n",
    "        # ones() makes it easy to write unit tests\n",
    "        w = tf.Variable( np.ones(( n, 1 )), name=\"w\")\n",
    "        b = tf.Variable( 1.0, name=\"b\", dtype=tf.float64)\n",
    "    else:\n",
    "        # usally machine learning algorithms init values using a uniform\n",
    "        # random distribution. Logistic regression typically uses zero.\n",
    "        # This should make learning faster. Has to do with the \n",
    "        # shape of the \n",
    "        \n",
    "        # sigmoid function\n",
    "        w = tf.Variable( np.zeros(( n, 1 )), name=\"w\")\n",
    "        b = tf.Variable( 0., name=\"b\", dtype=tf.float64)\n",
    "    \n",
    "print(\"w.shape: \", w.shape)\n",
    "print(\"tf.transpose(w).shape: \", tf.transpose(w).shape)\n",
    "\n",
    "print(\"b.dtype: \", b.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute z: the input to Sigmoid()\n",
    "$$ \n",
    "z\\: =\\: { W }^{ t }X\\: +\\: b \\tag{1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prediction\n",
    "Compute z: the input to Sigmoid()\n",
    "$$ \n",
    "z\\: =\\: { W }^{ t }X\\: +\\: b \\tag{1}\n",
    "$$\n",
    "\n",
    "Compute the \"neuron activation\"\n",
    "$$ \n",
    "a \\: = \\:  \\sigma (z)\\: =\\: 1\\, /\\, (1\\: +\\: { e }^{ -z }) \\tag{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Predict\"):\n",
    "    # define forward propagation\n",
    "    # z has shape [1,m]\n",
    "    z = tf.matmul(tf.transpose(w), X) + b\n",
    "    tf.identity(z, \"z\")\n",
    "\n",
    "    # sigmoid has shape [1,m]\n",
    "    sigmoid = 1.0 / (1.0 + tf.exp( -1.0 * z ))\n",
    "    tf.identity(sigmoid, \"sigmoid\")\n",
    "    \n",
    "    a = sigmoid\n",
    "    tf.identity(a, \"a\")\n",
    "    \n",
    "    # define the error_rate test statistic and a TensorBoard summary \n",
    "    # yhat is our estimate of y given our current model\n",
    "    yhat = tf.cast(a > 0.5, a.dtype)\n",
    "    tf.identity(yhat, \"yhat\")\n",
    "    \n",
    "    match = tf.cast( tf.equal(yhat, y), yhat.dtype ) \n",
    "    error_rate = 1.0 - tf.reduce_mean( match, axis=1 )\n",
    "    tf.identity(error_rate, \"error_rate\")\n",
    "    error_rate_summary = tf.summary.scalar('Error_rate', error_rate[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG: # test eq. (1)\n",
    "    with tf.Session() as sess:\n",
    "        init = tf.global_variables_initializer()\n",
    "        init.run() # actual init of variable    \n",
    "        result = z.eval( feed_dict={X:X_train , y:y_train} )\n",
    "        \n",
    "    assert ( result.shape == (1,m) )\n",
    "        \n",
    "    def debugExpectedZ():\n",
    "        return  np.array( [[ 4.,  8., 12., 16., 20., 24., 28.]] )\n",
    "    \n",
    "    assert np.array_equal( result, debugExpectedZ() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrTest.debugZ(m, z, {X:X_train, y:y_train}, DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG: # test eq. (2)\n",
    "    with tf.Session() as sess:\n",
    "        init = tf.global_variables_initializer()\n",
    "        init.run() # actual init of variable    \n",
    "        result = a.eval( feed_dict={X:X_train , y:y_train} )\n",
    "        \n",
    "    assert (result.shape == (1,m) )\n",
    "\n",
    "    def debugExpectedSigmoid():\n",
    "        zz = debugExpectedZ()\n",
    "        expected = 1.0 / ( 1.0 + np.exp(-1.0 * zz) )\n",
    "        return expected\n",
    "        \n",
    "    assert np.array_equal( result, debugExpectedSigmoid() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG: # test error_rate\n",
    "    with tf.Session() as sess:\n",
    "        init = tf.global_variables_initializer()\n",
    "        init.run() # actual init of variable    \n",
    "        #result = error_rate.eval( feed_dict={X:X_train , y:y_train} )\n",
    "        fetchResults = {\n",
    "            \"yhat\":yhat,\n",
    "            \"match\":match,\n",
    "            \"error_rate\":error_rate\n",
    "        }\n",
    "        \n",
    "        results = sess.run(fetchResults, \n",
    "                          feed_dict={X:X_train , y:y_train} )\n",
    "        aa = a.eval(feed_dict={X:X_train , y:y_train})\n",
    "        expected = 1 - np.sum((aa > 0.5) == y_train) / m\n",
    "        assert results['error_rate'] == expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model\n",
    "Create a graph to measure how well our preidiction match our lables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Equation\n",
    "$$ \n",
    "L(a ,y)\\: =\\: -\\,(\\: ylog(a)\\: +\\: (1-y)log(1-a )\\: ) \\tag{3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Equations\n",
    "\n",
    "$$ \n",
    "J(W,b)\\: =\\: 1/m\\, \\sum _{ i=1 }^{ m }{ L(a ^{ (i) } } ,{ y }^{ (i) }) \\tag{4}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Cost\"):\n",
    "    # define loss function. Shape will be [1,m]\n",
    "\n",
    "    # use member wise multiplication\n",
    "    propY = tf.multiply( y, tf.log(a) )\n",
    "    propNotY = tf.multiply( (1.0 - y), tf.log(1.0 - a) )\n",
    "    loss = - ( propY + propNotY )\n",
    "    tf.identity(loss, \"loss\")\n",
    "    \n",
    "    # define cost. i.e. average loss. Shape will be a real number\n",
    "    # and a TensorBoard summary \n",
    "\n",
    "    cost = 1/m * tf.reduce_sum( loss )\n",
    "    tf.identity(cost, \"cost\")\n",
    "    cost_summary = tf.summary.scalar('Cost', cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    # test eq. (3)\n",
    "    with tf.Session() as sess:\n",
    "        init = tf.global_variables_initializer()\n",
    "        init.run() # actual init of variable   \n",
    "        result = loss.eval( feed_dict={X:X_train , y:y_train} )\n",
    "        \n",
    "    assert (result.shape == (1,m) )\n",
    "        \n",
    "    def debugExpectedLoss():\n",
    "        aa = debugExpectedSigmoid()\n",
    "        prob = y_train * np.log( aa )\n",
    "        probNot = (1.0 - y_train) * np.log( 1.0 - aa )\n",
    "        expected = -1.0 * ( prob + probNot )\n",
    "        return expected\n",
    "        \n",
    "    expected = debugExpectedLoss()\n",
    "    assert np.array_equal( result, expected )\n",
    "\n",
    "    # test eq. (4)\n",
    "    with tf.Session() as sess:\n",
    "        init = tf.global_variables_initializer()\n",
    "        init.run() # actual init of variable   \n",
    "        result = cost.eval( feed_dict={X:X_train , y:y_train} )\n",
    "    \n",
    "    def debugCost():\n",
    "        ll = debugExpectedLoss()\n",
    "        expected = 1.0/m * np.sum(ll, axis=1)\n",
    "        return expected\n",
    "        \n",
    "    r = np.round( result, decimals=6 )\n",
    "    d = np.round( debugCost(), decimals=6 )\n",
    "    assert (r == d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial derivatives of cost function with respect to weights\n",
    "\n",
    "$$ \\frac{\\partial J(w,b)}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial derivatives of cost function with respect to bias\n",
    "$$ \\frac{\\partial J(w,b)}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"back_prop\"):\n",
    "    # Shape will be (n,1)\n",
    "    dz = a - y\n",
    "    tf.identity(dz, name=\"dz\")\n",
    "    \n",
    "    dw = 1.0 / m * tf.matmul(X, tf.transpose(dz))\n",
    "    tf.identity(dw, name=\"dw\")\n",
    "\n",
    "    # db is a scalar\n",
    "    db = 1.0 / m * tf.reduce_sum( dz )\n",
    "    tf.identity(db, name=\"db\")\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    update_w = tf.assign(w, w - learning_rate * dw, name=\"update_w\")\n",
    "    update_b = tf.assign(b, b - learning_rate * db, name=\"update_b\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if DEBUG: \n",
    "    with tf.name_scope(\"debug_gradient\"):\n",
    "        # test eq. (7)\n",
    "        debug_reset_w = tf.assign( w, np.ones(( n, 1 )) )\n",
    "        epsilon = 0.0001\n",
    "        batch = {X:X_train, y:y_train}\n",
    "\n",
    "        def debugGradientCheck():\n",
    "            \"\"\"uses definittion of derivative to compute a double side\n",
    "            estimate of cost to check gradident\"\"\"\n",
    "\n",
    "            with tf.Session() as sess:\n",
    "                init = tf.global_variables_initializer()\n",
    "                init.run()\n",
    "                # get the partial derivate as calculated by our \n",
    "                # computation graph\n",
    "                dwResult = dw.eval( feed_dict=batch )\n",
    "\n",
    "            def debugEstimateCost(rowIdx):\n",
    "                default = 1.\n",
    "                posAdj = (default + epsilon/2.0)\n",
    "                negAdj = (default + -1. * epsilon/2.0)\n",
    "                if rowIdx == 0:\n",
    "                    plusEps  = np.array( [[posAdj], [default]] )\n",
    "                    minusEps = np.array( [[negAdj], [default]] )\n",
    "                else:\n",
    "                    plusEps  = np.array( [[default], [posAdj]] )\n",
    "                    minusEps = np.array( [[default], [negAdj]] )\n",
    "\n",
    "                def debugCost(adj):\n",
    "                    with tf.Session() as sess:\n",
    "                        init = tf.global_variables_initializer()\n",
    "                        init.run() \n",
    "                        # tweak the values of w such that they will be \n",
    "                        # shared by the cost caluclation. We can use eval\n",
    "                        # to share variables\n",
    "                        fetchResults = [tf.assign(w,adj), cost]\n",
    "                        _w, new_cost = sess.run(fetchResults,\n",
    "                                                feed_dict=batch)\n",
    "                        return new_cost\n",
    "\n",
    "                upper_cost = debugCost( plusEps )\n",
    "                lower_cost = debugCost( minusEps )\n",
    "\n",
    "                estimate = (upper_cost - lower_cost) / epsilon\n",
    "                return estimate\n",
    "\n",
    "            g1 = debugEstimateCost(0)\n",
    "            g2 = debugEstimateCost(1)\n",
    "            estimated_gradients = np.array( [[g1], [g2]] )\n",
    "            return [dwResult, estimated_gradients]\n",
    "\n",
    "\n",
    "        dwResult, estimate = debugGradientCheck()\n",
    "        rr = np.round(dwResult, decimals=7)\n",
    "        gr = np.round(estimate, decimals=7)\n",
    "        assert (np.array_equal(rr, gr))\n",
    "\n",
    "        # as an additional check\n",
    "        # Use tensor flow Automatic Differentiation to test gradient\n",
    "        # Computing the gradient of cost with respect to W and b  \n",
    "    with tf.name_scope(\"debug_gradient\"):\n",
    "        with tf.Session() as sess:\n",
    "            init = tf.global_variables_initializer()\n",
    "            init.run()        \n",
    "\n",
    "            grad_w, grad_b = tf.gradients(xs=[w, b], ys=cost)\n",
    "            gradWResult = grad_w.eval(feed_dict={X:X_train, y:y_train})\n",
    "\n",
    "            # notice auto diff has greater precision than 2 sided estimate \n",
    "            rr = np.round(dwResult, decimals=11)\n",
    "            gwr = np.round(gradWResult, decimals=11)\n",
    "            assert (np.array_equal(rr, gwr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG: # test eq. (8)\n",
    "    with tf.Session() as sess:\n",
    "        init = tf.global_variables_initializer()\n",
    "        init.run() # actual init of variable  \n",
    "        batch = {X:X_train, y:y_train}\n",
    "        result = db.eval( feed_dict=batch )\n",
    "        \n",
    "        # Use tensor flow Automatic Differentiation to test gradient\n",
    "        # Computing the gradient of cost with respect to W and b\n",
    "        grad_w, grad_b = tf.gradients(xs=[w, b], ys=cost)\n",
    "        gBResult = grad_b.eval(feed_dict=batch)\n",
    "        \n",
    "\n",
    "        # notice greater precision than 2 sided estimate test\n",
    "        rr = np.round(result, decimals=11)\n",
    "        gbr = np.round(gBResult, decimals=11)\n",
    "        assert (np.array_equal(rr, gbr))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetchBatch(X, y, batchIndex, batchSize ):\n",
    "    start = batchIndex * batchSize\n",
    "    end = start + batchSize\n",
    "    xBatch = X[:,start:end]\n",
    "    yBatch = y[:,start:end] \n",
    "    return xBatch, yBatch\n",
    "\n",
    "if DEBUG:\n",
    "    with tf.Session() as sess, tf.name_scope(\"train\"):\n",
    "        init = tf.global_variables_initializer()\n",
    "        init.run() # actual init of variable   \n",
    "        \n",
    "        def testFetchBatch( batchIndex, batchSize, debug=False ):\n",
    "            Xb, yb = fetchBatch(X_train, y_train, batchIndex,\n",
    "                                batchSize )\n",
    "            return Xb, yb\n",
    "\n",
    "        Xb, yb = testFetchBatch( batchIndex=0, batchSize=2)\n",
    "        assert np.array_equal( Xb, [[1., 3.], [2., 4.]] )\n",
    "        assert np.array_equal( yb, [[0., 0.]] )\n",
    "\n",
    "        Xb, yb = testFetchBatch( batchIndex=1, batchSize=2)\n",
    "        assert np.array_equal( Xb, [[5., 7.], [6., 8.]] )\n",
    "        assert np.array_equal( yb, [[0., 1.]] )\n",
    "\n",
    "        # there are 4 patch, the index of last batch is 3 an only \n",
    "        # has 1 row\n",
    "        Xb, yb = testFetchBatch( batchIndex=3, batchSize=2)\n",
    "        assert np.array_equal( Xb, [[13.], [14.]] )\n",
    "        assert np.array_equal( yb, [[1.]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   0 totalCost:0.56703763 change:0.56703763 error rate:0.06700\n",
      "epoch:  20 totalCost:0.15448761 change:-0.00292511 error rate:0.03700\n",
      "epoch:  40 totalCost:0.11653076 change:-0.00135690 error rate:0.03300\n",
      "epoch:  60 totalCost:0.09495562 change:-0.00088699 error rate:0.03200\n",
      "epoch:  80 totalCost:0.07996599 change:-0.00064538 error rate:0.02800\n",
      "\n",
      "m: 1000  epochs: 100\n",
      "final error_rate: [0.028]\n",
      "logdir: tf_logs/run-2018_01_30_01:51:55/\n",
      "CPU times: user 2min 27s, sys: 48.6 s, total: 3min 15s\n",
      "Wall time: 3min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from datetime import datetime\n",
    "\n",
    "# easy of use combine all the summaries \n",
    "merged_summary = tf.summary.merge_all()\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# set up logging so we can use TensorBoard to analyze results\n",
    "now = datetime.utcnow().strftime(\"%Y_%m_%d_%H:%M:%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "    \n",
    "# it appears that stats are collected for each mini batch\n",
    "# how ever they are only written to disk when summary_writer.add()\n",
    "# is executed.\n",
    "summary_writer = tf.summary.FileWriter(logdir)\n",
    "summary_writer.add_graph( tf.get_default_graph() )\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run() # actual init of variable\n",
    "    previousCost = 0.\n",
    "    for epoch in range( epochs ):\n",
    "        totalCost = 0.0\n",
    "        numBatches = int( np.ceil(m / batch_size) )\n",
    "        for i in range( numBatches):   \n",
    "            if i % log_frequence == 0:\n",
    "                stat_data = { X:X_train, y:y_train }\n",
    "                step = epoch * numBatches + i\n",
    "                log_entry = sess.run(merged_summary, \n",
    "                                     feed_dict=stat_data)\n",
    "                summary_writer.add_summary(log_entry, step)\n",
    "                \n",
    "            # train using next mini batch\n",
    "            xBatch, yBatch = fetchBatch( X_train, y_train,\n",
    "                                        batchIndex=i,\n",
    "                                        batchSize=batch_size )\n",
    "            \n",
    "            data = { X:xBatch, y:yBatch}\n",
    "            fetchResults = {\n",
    "                        \"cost\":cost,\n",
    "                        \"error_rate\":error_rate,\n",
    "                        # the update ops are the root of our graph\n",
    "                        \"update_w\":update_w,\n",
    "                        \"update_b\":update_b\n",
    "            }\n",
    "                \n",
    "            results = sess.run(fetchResults, feed_dict=data)\n",
    "            totalCost += results[\"cost\"]\n",
    "                    \n",
    "        # we expect the change to be negative if our model \n",
    "        # is improving\n",
    "        change = totalCost - previousCost \n",
    "        previousCost = totalCost\n",
    "        \n",
    "        if ((epoch % log_frequence) == 0):             \n",
    "            data = { X:X_train, y:y_train}\n",
    "            answer = {\"error_rate\":error_rate}\n",
    "            \n",
    "            totalResults = sess.run(answer, feed_dict=data)\n",
    "            fmt = \"epoch:{:>4} totalCost:{:,.8f} change:{:,.8f}\" + \\\n",
    "                    \" error rate:{:,.5f}\"\n",
    "            print(fmt.format(epoch, totalCost, change, \n",
    "                             totalResults[\"error_rate\"][0]) )\n",
    "        \n",
    "    print(\"\\nm:\", m, \" epochs:\", epochs)\n",
    "    print(\"final error_rate:\", totalResults[\"error_rate\"] )\n",
    "    print(\"logdir:\", logdir)\n",
    "\n",
    "# close the log file\n",
    "summary_writer.close()\n",
    "    \n",
    "if DEBUG:    \n",
    "    msg = \"check epochs == 5000, learning_rate = 0.001\"\n",
    "    assert np.isclose(totalResults[\"error_rate\"], 0.28571429),msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
