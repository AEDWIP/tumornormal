{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "by Andrew E. Davidson\n",
    "\n",
    "This notebook is an introduction to TensorFlow. To illustrate how basic TensorFlow works we are going to implement binary classifier using Logistic Regression. Logistic Regression can be thought of as neural network with a single neuron. Its simple enough that we can implement the algorithm manually. With the interest of learning how tensor flow works we are not going to take advantage of automatic differentiation or tensoflow optimizers. Using auto-diff and optimizers would dramatically reduce the amount of code required.\n",
    "\n",
    "Note: [train.ipynb](train.ipynb) is a version of logistic regression using Keras. Its a fraction of the code a TensorFlow only solution requires.\n",
    "\n",
    "<span style=\"color:red\">There are couple of place where I used numpy reshape or to scale the feature matrix. This should be changed to use TensorFlow. Using numpy will work if you are using a single machine. Tensorflow will scale across a cluster.</span>\n",
    "    \n",
    "Supervised learning applications implemented with TensorFlow typically have the following structure\n",
    "1. load some data\n",
    "\n",
    "2. construct our 'computation graph'\n",
    "    1. Initialize our input variables, model parameters, and place holders\n",
    "    1. Forward Propagation\n",
    "        * create graph that makes predictions give our features and model\n",
    "    2. Calculate Cost\n",
    "        * create graph to measure how well our preidiction match our lables\n",
    "    3. Back Propagation\n",
    "        * calculate the graidents. (i.e. the partical derivatites of our cost function with respect to the partameters our model learns\n",
    "    4. use the gradients to update the learned parameters of our model\n",
    "3. Train our model\n",
    "    1. Use an optimizer or gradient decent with mini-batches\n",
    "    2. save check points and final model\n",
    "    3. save performance statistics. \n",
    "4. evaluate our model's performance on the training data set\n",
    "5. evaluate our model's performance on the test data set\n",
    "6. tune hyper parmeters \n",
    "7. goto to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/importlib/_bootstrap.py:205: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n",
      "/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import logRegTestFunc as lrTest\n",
    "import h5py\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "#from keras.utils.io_utils import HDF5Matrix\n",
    "from scipy import stats\n",
    "\n",
    "# import local files\n",
    "import sys\n",
    "sys.path.append('pyDevProj/src')\n",
    "import utils.load as utils\n",
    "import utils.preprocess as pre\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Load RNA Seq data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets: ['X_test', 'X_train', 'class_labels', 'classes_test', 'classes_train', 'features', 'genes', 'labels', 'y_test', 'y_train']\n",
      "Training on trival (debug) data set\n",
      "CPU times: user 1.45 ms, sys: 617 Âµs, total: 2.06 ms\n",
      "Wall time: 1.87 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "input_file = \"data/tumor_normal.h5\"\n",
    "\n",
    "with h5py.File(input_file, \"r\") as f:\n",
    "    print(\"Datasets:\", list(f.keys()))\n",
    "    \n",
    "DEBUG = True\n",
    "if DEBUG:\n",
    "    # this data set is simple enough we can do the math on paper\n",
    "    X_train, y_train = utils.test_train_data_set()\n",
    "    epochs = 500\n",
    "    batch_size = 3\n",
    "    log_frequence = 100\n",
    "    \n",
    "else:\n",
    "    X_train, y_train = utils.small_train_data_set(input_file)\n",
    "    #X_train, y_train = utils.full_train_data_set(input_file)\n",
    "    \n",
    "    n,m = np.transpose(X_train).shape\n",
    "    \n",
    "    # reshape array into a column vector\n",
    "    y_train = np.reshape(y_train, (1,m))\n",
    "\n",
    "    epochs=100\n",
    "    batch_size=128\n",
    "    log_frequence = epochs / 5 #1 if DEBUG else 20 \n",
    "    \n",
    "# transpose, each column should be a separate sample\n",
    "X_train = np.transpose( X_train )\n",
    "n,m = X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: num features:  2\n",
      "m: num training samples:  7\n",
      "epochs:  500\n",
      "batch_size:  3\n"
     ]
    }
   ],
   "source": [
    "print(\"n: num features: \", n)\n",
    "print(\"m: num training samples: \", m)\n",
    "print( \"epochs: \", epochs )\n",
    "print( \"batch_size: \", batch_size )\n",
    "\n",
    "utils.check_data(X_train, y_train, DEBUG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore RNA Seq data\n",
    "Given the way the sigmoid function works, the raw data will spend a lot of time learning using gradients near zero. We can speed up learning by normalizing the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    testND = np.arange(start=1., stop=13.).reshape(4,3)\n",
    "    result = pre.normalizeData(testND)\n",
    "\n",
    "    assert np.array_equal( np.mean(result, axis=1), \n",
    "                          np.array([0.,0.,0.,0.]) )\n",
    "    assert sum(np.isclose( np.var(result, axis=1), \n",
    "                          np.array([1.,1.,1.,1.]) ), 4)\n",
    "\n",
    "if not DEBUG:\n",
    "    # explore some of the X_train data\n",
    "    # do we need to normalize?\n",
    "    print(\"******** Explore RNA Seq data *******\")\n",
    "    print(\"Raw Data, rows are features\")\n",
    "    print(\"\\nX_train[0:2,0:8]\\n\", X_train[0:2,0:8])\n",
    "    print(\"\\nX_train[100:102,0:8]\\n\", X_train[100:102,0:8])\n",
    "    \n",
    "    print(\"\\ny_train[0, 0:8]\\n\", y_train[0, 0:8])\n",
    "\n",
    "    print(\"\\nSummary stats for two genes\")\n",
    "    s = stats.describe(X_train[0,:])\n",
    "    print(\"\\nX_train[0,:]\\n minmax:{} mean:{} variance:{}\"\n",
    "              .format(s.minmax, s.mean, s.variance) )\n",
    "    \n",
    "    s = stats.describe(X_train[99,:])\n",
    "    print(\"\\nX_train[99,:]\\n minmax:{} mean:{} variance:{}\"\n",
    "              .format(s.minmax, s.mean, s.variance) )  \n",
    "    \n",
    "    print(\"\\n********** normalizing data ********\")\n",
    "    X_train = pre.normalizeData( X_train )\n",
    "    print(\"AEDWIP !!!!! do not load test data\")\n",
    "    X_test = pre.normalizeData( X_test )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Construction Computation Graph \n",
    "If you want to use TensorBoard to visualize your graph make sure you give all ops a name, and group releated ops using \"name scopes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear out any nodes from previous runs\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation\n",
    "Create a graph that makes predictions given our features and model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init place holders and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape:  (2, ?)\n",
      "y.shape: (1, ?)\n",
      "w.shape:  (2, 1)\n",
      "tf.transpose(w).shape:  (1, 2)\n",
      "b.dtype:  <dtype: 'float64_ref'>\n"
     ]
    }
   ],
   "source": [
    "# We use tf.name_scope() to group tensor operations in the graph image \n",
    "# created by TensorBoard. Use either the \"name\" argument or tf.identity to \n",
    "# lable nodes in the in the TensorBoard Image\n",
    "\n",
    "with tf.name_scope(\"input_data\"):\n",
    "    # shape( None ) means the deminsion is not know. This is because the \n",
    "    # last min batch may have fewer samples. i.e. m / batch_size  may have \n",
    "    # a remainder\n",
    "\n",
    "    # X is our feature matrix\n",
    "    X = tf.placeholder( tf.float64, shape=(n, None), name=\"X\")\n",
    "    print(\"X.shape: \", X.shape)\n",
    "\n",
    "    # y is our lable vector\n",
    "    y = tf.placeholder( tf.float64, shape=(1, None), name=\"y\")\n",
    "    print(\"y.shape:\", y.shape)\n",
    "\n",
    "# initialize our model parameters.\n",
    "# w is our models weight vector\n",
    "# b is the y intercept for a linear model\n",
    "with tf.name_scope(\"model_parameters\"):\n",
    "    if DEBUG:\n",
    "        # ones() makes it easy to write unit tests\n",
    "        w = tf.Variable( np.ones(( n, 1 )), name=\"w\")\n",
    "        b = tf.Variable( 1.0, name=\"b\", dtype=tf.float64)\n",
    "    else:\n",
    "        # usally machine learning algorithms init values using a uniform\n",
    "        # random distribution. Logistic regression typically uses zero.\n",
    "        # This should make learning faster. Has to do with the \n",
    "        # shape of the \n",
    "        \n",
    "        # sigmoid function\n",
    "        w = tf.Variable( np.zeros(( n, 1 )), name=\"w\")\n",
    "        b = tf.Variable( 0., name=\"b\", dtype=tf.float64)\n",
    "    \n",
    "print(\"w.shape: \", w.shape)\n",
    "print(\"tf.transpose(w).shape: \", tf.transpose(w).shape)\n",
    "\n",
    "print(\"b.dtype: \", b.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute z: the input to Sigmoid()\n",
    "$$ \n",
    "z\\: =\\: { W }^{ t }X\\: +\\: b \\tag{1}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prediction\n",
    "Compute z: the input to Sigmoid()\n",
    "$$ \n",
    "z\\: =\\: { W }^{ t }X\\: +\\: b \\tag{1}\n",
    "$$\n",
    "\n",
    "Compute the \"neuron activation\"\n",
    "$$ \n",
    "a \\: = \\:  \\sigma (z)\\: =\\: 1\\, /\\, (1\\: +\\: { e }^{ -z }) \\tag{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Predict\"):\n",
    "    # define forward propagation\n",
    "    # z has shape [1,m]\n",
    "    z = tf.matmul(tf.transpose(w), X) + b\n",
    "    tf.identity(z, \"z\")\n",
    "\n",
    "    # sigmoid has shape [1,m]\n",
    "    sigmoid = 1.0 / (1.0 + tf.exp( -1.0 * z ))\n",
    "    tf.identity(sigmoid, \"sigmoid\")\n",
    "    \n",
    "    a = sigmoid\n",
    "    tf.identity(a, \"a\")\n",
    "    \n",
    "    # define the error_rate test statistic and a TensorBoard summary \n",
    "    # yhat is our estimate of y given our current model\n",
    "    yhat = tf.cast(a > 0.5, a.dtype)\n",
    "    tf.identity(yhat, \"yhat\")\n",
    "    \n",
    "    match = tf.cast( tf.equal(yhat, y), yhat.dtype ) \n",
    "    error_rate = 1.0 - tf.reduce_mean( match, axis=1 )\n",
    "    tf.identity(error_rate, \"error_rate\")\n",
    "    error_rate_summary = tf.summary.scalar('Error_rate', error_rate[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG: # test eq. (1)\n",
    "    with tf.Session() as sess:\n",
    "        init = tf.global_variables_initializer()\n",
    "        init.run() # actual init of variable    \n",
    "        result = z.eval( feed_dict={X:X_train , y:y_train} )\n",
    "        \n",
    "    assert ( result.shape == (1,m) )\n",
    "        \n",
    "    def debugExpectedZ():\n",
    "        return  np.array( [[ 4.,  8., 12., 16., 20., 24., 28.]] )\n",
    "    \n",
    "    assert np.array_equal( result, debugExpectedZ() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debugZ() passed\n"
     ]
    }
   ],
   "source": [
    "lrTest.debugZ(m, z, {X:X_train, y:y_train}, DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG: # test eq. (2)\n",
    "    with tf.Session() as sess:\n",
    "        init = tf.global_variables_initializer()\n",
    "        init.run() # actual init of variable    \n",
    "        result = a.eval( feed_dict={X:X_train , y:y_train} )\n",
    "        \n",
    "    assert (result.shape == (1,m) )\n",
    "\n",
    "    def debugExpectedSigmoid():\n",
    "        zz = debugExpectedZ()\n",
    "        expected = 1.0 / ( 1.0 + np.exp(-1.0 * zz) )\n",
    "        return expected\n",
    "        \n",
    "    assert np.array_equal( result, debugExpectedSigmoid() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG: # test error_rate\n",
    "    with tf.Session() as sess:\n",
    "        init = tf.global_variables_initializer()\n",
    "        init.run() # actual init of variable    \n",
    "        #result = error_rate.eval( feed_dict={X:X_train , y:y_train} )\n",
    "        fetchResults = {\n",
    "            \"yhat\":yhat,\n",
    "            \"match\":match,\n",
    "            \"error_rate\":error_rate\n",
    "        }\n",
    "        \n",
    "        results = sess.run(fetchResults, \n",
    "                          feed_dict={X:X_train , y:y_train} )\n",
    "        aa = a.eval(feed_dict={X:X_train , y:y_train})\n",
    "        expected = 1 - np.sum((aa > 0.5) == y_train) / m\n",
    "        assert results['error_rate'] == expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Model\n",
    "Create a graph to measure how well our preidiction match our lables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Equation\n",
    "$$ \n",
    "L(a ,y)\\: =\\: -\\,(\\: ylog(a)\\: +\\: (1-y)log(1-a )\\: ) \\tag{3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Equations\n",
    "\n",
    "$$ \n",
    "J(W,b)\\: =\\: 1/m\\, \\sum _{ i=1 }^{ m }{ L(a ^{ (i) } } ,{ y }^{ (i) }) \\tag{4}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Cost\"):\n",
    "    # define loss function. Shape will be [1,m]\n",
    "\n",
    "    # use member wise multiplication\n",
    "    propY = tf.multiply( y, tf.log(a) )\n",
    "    propNotY = tf.multiply( (1.0 - y), tf.log(1.0 - a) )\n",
    "    loss = - ( propY + propNotY )\n",
    "    tf.identity(loss, \"loss\")\n",
    "    \n",
    "    # define cost. i.e. average loss. Shape will be a real number\n",
    "    # and a TensorBoard summary \n",
    "\n",
    "    cost = 1/m * tf.reduce_sum( loss )\n",
    "    tf.identity(cost, \"cost\")\n",
    "    cost_summary = tf.summary.scalar('Cost', cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    # test eq. (3)\n",
    "    with tf.Session() as sess:\n",
    "        init = tf.global_variables_initializer()\n",
    "        init.run() # actual init of variable   \n",
    "        result = loss.eval( feed_dict={X:X_train , y:y_train} )\n",
    "        \n",
    "    assert (result.shape == (1,m) )\n",
    "        \n",
    "    def debugExpectedLoss():\n",
    "        aa = debugExpectedSigmoid()\n",
    "        prob = y_train * np.log( aa )\n",
    "        probNot = (1.0 - y_train) * np.log( 1.0 - aa )\n",
    "        expected = -1.0 * ( prob + probNot )\n",
    "        return expected\n",
    "        \n",
    "    expected = debugExpectedLoss()\n",
    "    assert np.array_equal( result, expected )\n",
    "\n",
    "    # test eq. (4)\n",
    "    with tf.Session() as sess:\n",
    "        init = tf.global_variables_initializer()\n",
    "        init.run() # actual init of variable   \n",
    "        result = cost.eval( feed_dict={X:X_train , y:y_train} )\n",
    "    \n",
    "    def debugCost():\n",
    "        ll = debugExpectedLoss()\n",
    "        expected = 1.0/m * np.sum(ll, axis=1)\n",
    "        return expected\n",
    "        \n",
    "    r = np.round( result, decimals=6 )\n",
    "    d = np.round( debugCost(), decimals=6 )\n",
    "    assert (r == d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial derivatives of cost function with respect to weights\n",
    "\n",
    "$$ \\frac{\\partial J(w,b)}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial derivatives of cost function with respect to bias\n",
    "$$ \\frac{\\partial J(w,b)}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"back_prop\"):\n",
    "    # Shape will be (n,1)\n",
    "    dz = a - y\n",
    "    tf.identity(dz, name=\"dz\")\n",
    "    \n",
    "    dw = 1.0 / m * tf.matmul(X, tf.transpose(dz))\n",
    "    tf.identity(dw, name=\"dw\")\n",
    "\n",
    "    # db is a scalar\n",
    "    db = 1.0 / m * tf.reduce_sum( dz )\n",
    "    tf.identity(db, name=\"db\")\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    update_w = tf.assign(w, w - learning_rate * dw, name=\"update_w\")\n",
    "    update_b = tf.assign(b, b - learning_rate * db, name=\"update_b\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if DEBUG: \n",
    "    with tf.name_scope(\"debug_gradient\"):\n",
    "        # test eq. (7)\n",
    "        debug_reset_w = tf.assign( w, np.ones(( n, 1 )) )\n",
    "        epsilon = 0.0001\n",
    "        batch = {X:X_train, y:y_train}\n",
    "\n",
    "        def debugGradientCheck():\n",
    "            \"\"\"uses definittion of derivative to compute a double side\n",
    "            estimate of cost to check gradident\"\"\"\n",
    "\n",
    "            with tf.Session() as sess:\n",
    "                init = tf.global_variables_initializer()\n",
    "                init.run()\n",
    "                # get the partial derivate as calculated by our \n",
    "                # computation graph\n",
    "                dwResult = dw.eval( feed_dict=batch )\n",
    "\n",
    "            def debugEstimateCost(rowIdx):\n",
    "                default = 1.\n",
    "                posAdj = (default + epsilon/2.0)\n",
    "                negAdj = (default + -1. * epsilon/2.0)\n",
    "                if rowIdx == 0:\n",
    "                    plusEps  = np.array( [[posAdj], [default]] )\n",
    "                    minusEps = np.array( [[negAdj], [default]] )\n",
    "                else:\n",
    "                    plusEps  = np.array( [[default], [posAdj]] )\n",
    "                    minusEps = np.array( [[default], [negAdj]] )\n",
    "\n",
    "                def debugCost(adj):\n",
    "                    with tf.Session() as sess:\n",
    "                        init = tf.global_variables_initializer()\n",
    "                        init.run() \n",
    "                        # tweak the values of w such that they will be \n",
    "                        # shared by the cost caluclation. We can not \n",
    "                        # use eval to share variables\n",
    "                        fetchResults = [tf.assign(w,adj), cost]\n",
    "                        _w, new_cost = sess.run(fetchResults,\n",
    "                                                feed_dict=batch)\n",
    "                        return new_cost\n",
    "\n",
    "                upper_cost = debugCost( plusEps )\n",
    "                lower_cost = debugCost( minusEps )\n",
    "\n",
    "                estimate = (upper_cost - lower_cost) / epsilon\n",
    "                return estimate\n",
    "\n",
    "            g1 = debugEstimateCost(0)\n",
    "            g2 = debugEstimateCost(1)\n",
    "            estimated_gradients = np.array( [[g1], [g2]] )\n",
    "            return [dwResult, estimated_gradients]\n",
    "\n",
    "\n",
    "        dwResult, estimate = debugGradientCheck()\n",
    "        rr = np.round(dwResult, decimals=7)\n",
    "        gr = np.round(estimate, decimals=7)\n",
    "        assert (np.array_equal(rr, gr))\n",
    "\n",
    "        # as an additional check\n",
    "        # Use tensor flow Automatic Differentiation to test gradient\n",
    "        # Computing the gradient of cost with respect to W and b  \n",
    "    with tf.name_scope(\"debug_gradient\"):\n",
    "        with tf.Session() as sess:\n",
    "            init = tf.global_variables_initializer()\n",
    "            init.run()        \n",
    "\n",
    "            grad_w, grad_b = tf.gradients(xs=[w, b], ys=cost)\n",
    "            gradWResult = grad_w.eval(feed_dict={X:X_train, y:y_train})\n",
    "\n",
    "            # notice auto diff has greater precision than 2 sided estimate \n",
    "            rr = np.round(dwResult, decimals=11)\n",
    "            gwr = np.round(gradWResult, decimals=11)\n",
    "            assert (np.array_equal(rr, gwr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG: # test eq. (8)\n",
    "    with tf.Session() as sess:\n",
    "        init = tf.global_variables_initializer()\n",
    "        init.run() # actual init of variable  \n",
    "        batch = {X:X_train, y:y_train}\n",
    "        result = db.eval( feed_dict=batch )\n",
    "        \n",
    "        # Use tensor flow Automatic Differentiation to test gradient\n",
    "        # Computing the gradient of cost with respect to W and b\n",
    "        grad_w, grad_b = tf.gradients(xs=[w, b], ys=cost)\n",
    "        gBResult = grad_b.eval(feed_dict=batch)\n",
    "        \n",
    "\n",
    "        # notice greater precision than 2 sided estimate test\n",
    "        rr = np.round(result, decimals=11)\n",
    "        gbr = np.round(gBResult, decimals=11)\n",
    "        assert (np.array_equal(rr, gbr))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fetchBatch() TODO\n",
    "when we run on full training set run is really long. Also very high memory presure. Test to see if memory preasure is becuase of fetchBatch or computation?\n",
    "\n",
    "if presure is from batch maybe instead of keeping all of training set in memory we can read mini batch from disk. Does data package have read with start and end? If not we could pre process traing set into mini batches\n",
    "\n",
    "Give that set is composed from three different sources we should consider selecting random mini batches so that each mini batch is more likely to look like the over all data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetchBatch(X, y, batchIndex, batchSize ):\n",
    "    start = batchIndex * batchSize\n",
    "    end = start + batchSize\n",
    "    xBatch = X[:,start:end]\n",
    "    yBatch = y[:,start:end] \n",
    "    return xBatch, yBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    with tf.Session() as sess, tf.name_scope(\"train\"):\n",
    "        init = tf.global_variables_initializer()\n",
    "        init.run() # actual init of variable   \n",
    "        \n",
    "        def testFetchBatch( batchIndex, batchSize, debug=False ):\n",
    "            Xb, yb = fetchBatch(X_train, y_train, batchIndex,\n",
    "                                batchSize )\n",
    "            return Xb, yb\n",
    "\n",
    "        Xb, yb = testFetchBatch( batchIndex=0, batchSize=2)\n",
    "        assert np.array_equal( Xb, [[1., 3.], [2., 4.]] )\n",
    "        assert np.array_equal( yb, [[0., 0.]] )\n",
    "\n",
    "        Xb, yb = testFetchBatch( batchIndex=1, batchSize=2)\n",
    "        assert np.array_equal( Xb, [[5., 7.], [6., 8.]] )\n",
    "        assert np.array_equal( yb, [[0., 1.]] )\n",
    "\n",
    "        # there are 4 patch, the index of last batch is 3 an only \n",
    "        # has 1 row\n",
    "        Xb, yb = testFetchBatch( batchIndex=3, batchSize=2)\n",
    "        assert np.array_equal( Xb, [[13.], [14.]] )\n",
    "        assert np.array_equal( yb, [[1.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easy of use: combine all the summaries \n",
    "merged_summary = tf.summary.merge_all()\n",
    "\n",
    "model_saver = tf.train.Saver()\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_name: models/logisticRegression/simpleLogisticRegression/2018_02_17_01:24:48/model\n"
     ]
    }
   ],
   "source": [
    "# set up logging so we can use TensorBoard to analyze results\n",
    "from datetime import datetime\n",
    "\n",
    "now = datetime.utcnow().strftime(\"%Y_%m_%d_%H:%M:%S\")\n",
    "root_logdir = \"tf_logs\"\n",
    "logdir = \"{}/run-{}/\".format(root_logdir, now)\n",
    "\n",
    "model_dir = \"models/logisticRegression\"\n",
    "model_name = \"{}/simpleLogisticRegression/{}/model\".format(model_dir, now)\n",
    "print(\"model_name:\", model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 134 ms, sys: 3.12 ms, total: 137 ms\n",
      "Wall time: 136 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# it appears that stats are collected for each mini batch\n",
    "# how ever they are only written to disk when summary_writer.add()\n",
    "# is executed.\n",
    "summary_writer = tf.summary.FileWriter(logdir)\n",
    "summary_writer.add_graph( tf.get_default_graph() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:   0 totalCost:3.43121309 change:3.43121309 error rate:0.42857\n",
      "epoch: 100 totalCost:2.95728493 change:-0.00472883 error rate:0.42857\n",
      "epoch: 200 totalCost:2.48597448 change:-0.00469424 error rate:0.42857\n",
      "epoch: 300 totalCost:2.01952291 change:-0.00462772 error rate:0.42857\n",
      "epoch: 400 totalCost:1.56311102 change:-0.00448054 error rate:0.42857\n",
      "\n",
      "m: 7  epochs: 500\n",
      "final error_rate: [0.42857143]\n",
      "logdir: tf_logs/run-2018_02_17_01:24:48/\n",
      "path to file model: models/logisticRegression/simpleLogisticRegression/2018_02_17_01:24:48/model.tfModel\n",
      "CPU times: user 1min 32s, sys: 1.23 s, total: 1min 33s\n",
      "Wall time: 1min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run() # actual init of variable\n",
    "    previousCost = 0.\n",
    "    for epoch in range( epochs ):\n",
    "        totalCost = 0.0\n",
    "        numBatches = int( np.ceil(m / batch_size) )\n",
    "        for i in range( numBatches):   \n",
    "            if i % log_frequence == 0:\n",
    "                # save a check point\n",
    "                model_saver.save(sess, model_name + \".modelCkPt\")\n",
    "\n",
    "                # save test stats for entire data set\n",
    "                stat_data = { X:X_train, y:y_train }\n",
    "                step = epoch * numBatches + i\n",
    "                log_entry = sess.run(merged_summary, \n",
    "                                     feed_dict=stat_data)\n",
    "                summary_writer.add_summary(log_entry, step)\n",
    "                \n",
    "            # train using next mini batch\n",
    "            xBatch, yBatch = fetchBatch( X_train, y_train,\n",
    "                                        batchIndex=i,\n",
    "                                        batchSize=batch_size )\n",
    "            \n",
    "            data = { X:xBatch, y:yBatch}\n",
    "\n",
    "            fetchResults ={\n",
    "                            \"cost\":cost,\n",
    "                            \"error_rate\":error_rate,\n",
    "                            # the update ops are the root of our graph\n",
    "                            \"update_w\":update_w,\n",
    "                            \"update_b\":update_b\n",
    "                            }\n",
    "                \n",
    "            results = sess.run(fetchResults, feed_dict=data)\n",
    "            totalCost += results[\"cost\"]\n",
    "                    \n",
    "        # we expect the change to be negative if our model \n",
    "        # is improving\n",
    "        change = totalCost - previousCost \n",
    "        previousCost = totalCost\n",
    "        \n",
    "        if ((epoch % log_frequence) == 0):             \n",
    "            data = { X:X_train, y:y_train}\n",
    "            answer = {\"error_rate\":error_rate}\n",
    "            \n",
    "            totalResults = sess.run(answer, feed_dict=data)\n",
    "            fmt = \"epoch:{:>4} totalCost:{:,.8f} change:{:,.8f}\" + \\\n",
    "                    \" error rate:{:,.5f}\"\n",
    "            print(fmt.format(epoch, totalCost, change, \n",
    "                             totalResults[\"error_rate\"][0]) )\n",
    "        \n",
    "    print(\"\\nm:\", m, \" epochs:\", epochs)\n",
    "    print(\"final error_rate:\", totalResults[\"error_rate\"] )\n",
    "    print(\"logdir:\", logdir)\n",
    "    \n",
    "    # save final model\n",
    "    final_model_path = model_saver.save(sess, model_name + \".tfModel\")\n",
    "    print(\"path to file model:\", final_model_path)\n",
    "\n",
    "# close the log file\n",
    "summary_writer.close()\n",
    "   \n",
    "if DEBUG:    \n",
    "#     msg = \"check epochs == 5000, learning_rate = 0.001\"\n",
    "#     assert np.isclose(totalResults[\"error_rate\"], 0.28571429),msg\n",
    "    msg = \"check epochs == 500, learning_rate = 0.001\"\n",
    "    assert np.isclose(totalResults[\"error_rate\"], 0.42857),msg    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
